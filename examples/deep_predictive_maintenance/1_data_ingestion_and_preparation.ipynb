{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Ingestion & Preparation\n",
    "\n",
    "In this scenario, we build a LSTM network for the data set and scenario described at [Predictive Maintenance](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) to predict remaining useful life of aircraft engines. In summary, the scenario uses simulated aircraft values from 21 sensors to predict when an aircraft engine will fail in the future so that maintenance can be planned in advance.\n",
    "\n",
    "The data ingestion notebook will download the simulated predicitive maintenance data sets from a public Azure Blob Storage. Labels are created from the `truth` data and joined to the `training` and `test` data. After some preliminary data cleaning and verification, the results are stored in a local (to the notebook server) folder for use in the remaining notebooks of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from azureml.core import  (Workspace,Run,VERSION, \n",
    "                           Experiment,Datastore)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Azure ML service workspace & create experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = ''\n",
    "RESOURCE_GROUP = ''\n",
    "WORKSPACE_NAME = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = os.getcwd()\n",
    "exp_name = \"deep_predictive_maintenance\"\n",
    "\n",
    "\n",
    "ws = Workspace(workspace_name = WORKSPACE_NAME,\n",
    "               subscription_id = SUBSCRIPTION_ID ,\n",
    "               resource_group = RESOURCE_GROUP\n",
    "              )\n",
    "\n",
    "ws.write_config()\n",
    "\n",
    "print('Workspace loaded:', ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download simulated data sets\n",
    "We will be reusing the raw simulated data files from the [Predictive Maintenance](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) tutorial.\n",
    "\n",
    "The dataset has been made available [here](https://amlgitsamples.blob.core.windows.net/deeppredmaintenancedataset.zip) from which we will be loading data in cell below, the three data files are:\n",
    "\n",
    "    * `PM_train.txt`\n",
    "    * `PM_test.txt`\n",
    "    * `PM_truth.txt`\n",
    "    \n",
    "This notebook labels the train and test set and does some preliminary cleanup. We create some summary graphics for each data set to verify the data download, and store the resulting data sets in a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw train data is stored on Azure Blob here:\n",
    "basedataurl = \"https://amlgitsamples.blob.core.windows.net/deeppredmaintenance\"\n",
    "\n",
    "train_file_path = os.path.join(basedataurl,'PM_train.txt')\n",
    "test_file_path =  os.path.join(basedataurl,'PM_test.txt')\n",
    "truth_file_path  = os.path.join(basedataurl,'PM_truth.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Ingestion\n",
    "In the following section, we ingest the training, test and ground truth datasets from azure storage. The training data consists of multiple multivariate time series with `cycle` as the time unit, together with 21 sensor readings and 3 settings for each cycle. Each time series can be assumed as being generated from a different engine of the same type. The testing data has the same data schema as the training data. The only difference is that the data does not indicate when the failure occurs. Finally, the ground truth data provides the number of remaining working cycles for the engines in the testing data. You can find more details about the type of data used for this notebook at [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3).\n",
    "\n",
    "The training data consists of data from 100 engines (`id`) in the form of multivariate time series with `cycle` as the unit of time with 21 sensor readings `s1:s21` and 3 operational `setting` features for each `cycle`. In this simulated data, an engine is assumed to be operating normally at the start of each time series. Ebgine degradation progresses and grows in magnitude until a predefined threshold is reached where the engine is considered unsafe for further operation. In this simulation, the last cycle in each time series can be considered as the failure point of the corresponding engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw training data from Azure blob\n",
    "cols2drop = [26,27]\n",
    "\n",
    "# read training data \n",
    "train_df = pd.read_csv(train_file_path, sep=\" \", header=None)\n",
    "train_df.drop(train_df.columns[cols2drop], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                    's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                    's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "train_df.rename(columns={'id':'engine_id'}, inplace=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing data has the same data schema as the training data except the failure point is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_file_path, sep=\" \", header=None)\n",
    "test_df.drop(test_df.columns[cols2drop], axis=1, inplace=True)\n",
    "test_df.columns = train_df.columns\n",
    "test_df.rename(columns={'id':'engine_id'}, inplace=True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth data provides the number of remaining working cycles (Remaining useful life (RUL)) for the engines in the testing data. We use this data to evaluation the model after training with the training data set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ground truth data\n",
    "truth_df = pd.read_csv(truth_file_path, sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "\n",
    "truth_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We next generate labels for the training data. Since the last observation is assumed to be a failure point, we can calculate the Remaining Useful Life (`RUL`) for every cycle in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul = pd.DataFrame(train_df.groupby('engine_id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['engine_id', 'max']\n",
    "train_df = train_df.merge(rul, on=['engine_id'], how='left')\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "train_df['RUL'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RUL, we can create a label indicating time to failure. We define a boolean (`True\\False`) value for `label1` indicating the engine will fail within 30 days (RUL $<= 30$). We can also define a multiclass `label2` $\\in \\{0, 1, 2\\}$ indicating {Healthy, RUL <=30, RUL <=15} cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate label columns for training data\n",
    "nb_cycles = 15\n",
    "\n",
    "# Label1 indicates a failure will occur within the next 30 cycles.\n",
    "# 1 indicates failure, 0 indicates healthy \n",
    "train_df['label1'] = np.where(train_df['RUL'] <= nb_cycles*2, 1, 0 )\n",
    "\n",
    "# label2 is multiclass, value 1 is identical to label1,\n",
    "# value 2 indicates failure within 15 cycles\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <=nb_cycles, 'label2'] = 2\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) , cycle column is also used for training so we will also include the cycle column. Here, we normalize the columns in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax normalization\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['engine_id','cycle','RUL','label1','label2'])\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare the test data. We normalize the data using the same parameters from the training data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the ground truth dataset to generate labels for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate column max for test data\n",
    "rul = pd.DataFrame(test_df.groupby('engine_id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['engine_id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['engine_id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "\n",
    "# generate RUL for test data\n",
    "test_df = test_df.merge(truth_df, on=['engine_id'], how='left')\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the same labels as used for the `training` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate label columns for test data\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= nb_cycles*2, 1, 0 )\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= nb_cycles, 'label2'] = 2\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "One critical advantage of LSTMs is their ability to remember from long-term sequences (window sizes) which is hard to achieve by traditional feature engineering as computing rolling averages over large window sizes (i.e. 50 cycles) may lead to loss of information due to smoothing and abstracting of values over such a long period. While feature engineering over large window sizes may not make sense, LSTMs are able to use all the information in the window as input.\n",
    "\n",
    "We first look at an example of the sensor values for 50 cycles prior to the failure for engine `id = 3`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for visualizations \n",
    "# window of 50 cycles prior to a failure point for engine id 3\n",
    "engine_id3 = test_df[test_df['engine_id'] == 3]\n",
    "engine_id3_50cycleWindow = engine_id3[engine_id3['RUL'] <= engine_id3['RUL'].min() + 50]\n",
    "cols1 = ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10']\n",
    "engine_id3_50cycleWindow1 = engine_id3_50cycleWindow[cols1]\n",
    "\n",
    "# plotting sensor data for engine ID 3 prior to a failure point - sensors 1-10 \n",
    "ax1 = engine_id3_50cycleWindow1.plot(subplots=True, sharex=True, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting sensor data for engine ID 3 prior to a failure point - sensors 11-21 \n",
    "cols2 = ['s11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "engine_id3_50cycleWindow2 = engine_id3_50cycleWindow[cols2]\n",
    "ax2 = engine_id3_50cycleWindow2.plot(subplots=True, sharex=True, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we reshape the dataset from 2D dataframe into tensors into shape (samples, time steps, features) where samples is the number of training sequences, time steps is the look back window or sequence length and features is the number of features of each sequence at each time step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the data sets\n",
    "\n",
    "With the training and testing data created, we will save the training and testing datasets to disk, and we'll upload the training file to the datastore associated with our workspace so that it can be used on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(output_path, exist_ok = True)\n",
    "print('saving output to', output_path)\n",
    "\n",
    "train_data_path = os.path.join(output_path,'preprocessed_train_file.csv')\n",
    "test_data_path = os.path.join(output_path,'preprocessed_test_file.csv')\n",
    "\n",
    "train_df.to_csv(train_data_path, index=False)\n",
    "test_df.to_csv(test_data_path,index=False)\n",
    "\n",
    "\n",
    "ds = Datastore.get(ws,'workspaceblobstore')\n",
    "ds.upload_files(files = [train_data_path],\n",
    "                        target_path='data',\n",
    "                        overwrite=True,\n",
    "                        show_progress=True)\n",
    "\n",
    "\n",
    "datastore = {\"Data store name\" : ds.name,\n",
    "             'Account name' : ds.account_name,\n",
    "             'Blob container' : ds.container_name,\n",
    "            }\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(data = datastore, index = ['']).T"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:amlenv]",
   "language": "python",
   "name": "conda-env-amlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
